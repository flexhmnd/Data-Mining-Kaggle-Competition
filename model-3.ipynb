{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66b43f7d-f3d4-4c5f-90e8-cdbc1ffd64d9",
   "metadata": {},
   "source": [
    "# Model Building and Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49f0fa10-0231-4186-9fd5-3cdd1a565d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "training_data = pd.read_csv('data_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f45887de-d7a7-422a-8d3e-af7529a2a9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where 'text_clean' is NaN\n",
    "training_data = training_data.dropna(subset=['text_clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f58dc31-6bcb-4a58-ac3c-a97fe56b3dc0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get sentiment of tweets to help identify political views\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "def get_sentiment(text):\n",
    "    blob = TextBlob(text)\n",
    "    return blob.sentiment.polarity, blob.sentiment.subjectivity\n",
    "\n",
    "training_data[['sentiment_polarity', 'sentiment_subjectivity']] = training_data['text_clean'].apply(lambda x: pd.Series(get_sentiment(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dbc3eae9-6be0-4011-8d1b-403df5394c5e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Clean Features Shape: (405282, 674945)\n",
      "Full Text Features Shape: (405282, 947138)\n",
      "Hashtag Features Shape: (405282, 22574)\n",
      "Sentiment Features Shape: (405282, 8)\n",
      "Categorical Features Shape: (405282, 7)\n",
      "Reply Features Shape: (405282, 9913)\n",
      "Combined Features Shape: (405282, 1654595)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "\n",
    "# Step 1: Sentiment Features (Already Included)\n",
    "sentiment = csr_matrix(training_data[['sentiment_polarity', 'sentiment_subjectivity', 'tweet_length_chars', \n",
    "                                      'tweet_length_words', 'hashtag_length_chars', 'hashtag_length_words', \n",
    "                                      'text_clean_length_chars', 'text_clean_length_words']])\n",
    "\n",
    "# Normalize Sentiment Features\n",
    "sentiment_dense = sentiment.toarray()  # Convert to dense format for scaling\n",
    "scaler = MinMaxScaler()\n",
    "sentiment_normalized = scaler.fit_transform(sentiment_dense)\n",
    "sentiment = csr_matrix(sentiment_normalized)  # Convert back to sparse matrix\n",
    "\n",
    "# Step 2: Text Features (text_clean and full_text)\n",
    "# TF-IDF for 'text_clean'\n",
    "text_clean_vectorizer = TfidfVectorizer(ngram_range=(1, 2), stop_words=['english','swedish','norwegian','dutch','danish'], max_df=0.01, min_df=2)\n",
    "text_clean_features = text_clean_vectorizer.fit_transform(training_data['text_clean'])\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=10, random_state=42)\n",
    "topic_probs = lda.fit_transform(text_clean_features)\n",
    "# Add topic probabilities to features\n",
    "topic_features = csr_matrix(topic_probs)\n",
    "\n",
    "# TF-IDF for 'full_text'\n",
    "training_data['full_text'] = training_data['full_text'].fillna('')  # Replace NaN with an empty string\n",
    "full_text_vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_df=0.9, min_df=2)\n",
    "full_text_features = full_text_vectorizer.fit_transform(training_data['full_text'])\n",
    "\n",
    "# Step 3: Hashtag Features\n",
    "training_data['hashtags'] = training_data['hashtags'].fillna('')  # Replace NaN\n",
    "hashtag_vectorizer = TfidfVectorizer(ngram_range=(1, 2), stop_words='english', max_df=0.9, min_df=2)\n",
    "hashtag_features = hashtag_vectorizer.fit_transform(training_data['hashtags'])\n",
    "\n",
    "# Step 4: in_reply_to_screen_name Features\n",
    "# Replace NaN with 'missing'\n",
    "training_data['in_reply_to_screen_name'] = training_data['in_reply_to_screen_name'].fillna('missing')\n",
    "\n",
    "# If you prefer TF-IDF (if values are textual), you can uncomment:\n",
    "reply_vectorizer = TfidfVectorizer(ngram_range=(1, 1), stop_words='english', max_df=0.9, min_df=2)\n",
    "reply_features = reply_vectorizer.fit_transform(training_data['in_reply_to_screen_name'])\n",
    "\n",
    "# Step 5: Categorical Features (country_user and gender_user)\n",
    "categorical_encoder = OneHotEncoder(drop='first')\n",
    "categorical_features = categorical_encoder.fit_transform(training_data[['country_user', 'gender_user']])\n",
    "\n",
    "# Step 6: Combine All Features\n",
    "X = hstack([text_clean_features, full_text_features, hashtag_features, sentiment, categorical_features, reply_features, topic_features])\n",
    "\n",
    "# Define Target Variable\n",
    "y = training_data['pol_spec_user']  # Target variable\n",
    "\n",
    "# Print shapes for verification\n",
    "print(f\"Text Clean Features Shape: {text_clean_features.shape}\")\n",
    "print(f\"Full Text Features Shape: {full_text_features.shape}\")\n",
    "print(f\"Hashtag Features Shape: {hashtag_features.shape}\")\n",
    "print(f\"Sentiment Features Shape: {sentiment.shape}\")\n",
    "print(f\"Categorical Features Shape: {categorical_features.shape}\")\n",
    "print(f\"Reply Features Shape: {reply_features.shape}\")\n",
    "print(f\"Combined Features Shape: {X.shape}\")\n",
    "\n",
    "# Step 7: Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476bf1db-28b9-4892-a542-18c94568109d",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c3cf48f7-6c78-4d85-a336-33927e79eadc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7805\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Center       0.79      0.71      0.75     20862\n",
      " Independent       0.83      0.52      0.64       141\n",
      "        Left       0.78      0.84      0.80     34719\n",
      "       Right       0.78      0.76      0.77     25335\n",
      "\n",
      "    accuracy                           0.78     81057\n",
      "   macro avg       0.79      0.71      0.74     81057\n",
      "weighted avg       0.78      0.78      0.78     81057\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model = LogisticRegression(verbose=0,solver='liblinear', penalty='l2', max_iter=1000, C=10)  # Use 1000 iterations to ensure convergence\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd0c53e-a6ad-4fde-88d0-4fe360d71c05",
   "metadata": {},
   "source": [
    "#### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "34e438cf-5d4c-461f-a9a1-eae411c12dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]....................................................................................................\n",
      "optimization finished, #iter = 1000\n",
      "\n",
      "WARNING: reaching max number of iterations\n",
      "Using -s 2 may be faster (also see FAQ)\n",
      "\n",
      "Objective value = -49547.967386\n",
      "nSV = 160569\n",
      "....................................................................................................\n",
      "optimization finished, #iter = 1000\n",
      "\n",
      "WARNING: reaching max number of iterations\n",
      "Using -s 2 may be faster (also see FAQ)\n",
      "\n",
      "Objective value = -292.549843\n",
      "nSV = 1608\n",
      "....................................................................................................\n",
      "optimization finished, #iter = 1000\n",
      "\n",
      "WARNING: reaching max number of iterations\n",
      "Using -s 2 may be faster (also see FAQ)\n",
      "\n",
      "Objective value = -67209.825479\n",
      "nSV = 193862\n",
      "....................................................................................................\n",
      "optimization finished, #iter = 1000\n",
      "\n",
      "WARNING: reaching max number of iterations\n",
      "Using -s 2 may be faster (also see FAQ)\n",
      "\n",
      "Objective value = -54234.579387\n",
      "nSV = 170521\n",
      "SVM Accuracy: 0.7899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/svm/_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "model = LinearSVC(C=1, intercept_scaling=10, loss='hinge', verbose=1, max_iter=1000, dual=True)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred_svm = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "print(f\"SVM Accuracy: {accuracy_svm:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "118529b0-b8a1-471b-ba31-af214a681458",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_excel('test_data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9da68c2a-7dab-4e8d-b6b8-5eaecca30d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/felix.hammond/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/felix.hammond/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/felix.hammond/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from lemmatizer import lemmatize_tweet\n",
    "\n",
    "test_data['text_clean'] = test_data['full_text'].apply(lemmatize_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f651c3-cad4-481e-9057-cbfdc366e6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.to_csv('test_data_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "18eeddf5-6b3b-40a8-b586-c503973c91f9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "test_data['tweet_length_chars'] = test_data['full_text'].str.len()\n",
    "test_data['tweet_length_words'] = test_data['full_text'].str.split().str.len()\n",
    "\n",
    "# calculate hashtag length in characters and words\n",
    "test_data['hashtag_length_chars'] = test_data['hashtags'].fillna(\"\").str.len()\n",
    "test_data['hashtag_length_words'] = test_data['hashtags'].fillna(\"\").str.split().str.len()\n",
    "\n",
    "test_data['text_clean_length_chars'] = test_data['text_clean'].str.len() # character length\n",
    "test_data['text_clean_length_words'] = test_data['text_clean'].str.split().apply(len) # word length\n",
    "\n",
    "def get_sentiment(text):\n",
    "    blob = TextBlob(text)\n",
    "    return blob.sentiment.polarity, blob.sentiment.subjectivity\n",
    "\n",
    "test_data[['sentiment_polarity', 'sentiment_subjectivity']] = test_data['text_clean'].apply(lambda x: pd.Series(get_sentiment(x)))\n",
    "\n",
    "sentiment_test = csr_matrix(test_data[['sentiment_polarity', 'sentiment_subjectivity', 'tweet_length_chars', 'tweet_length_words',\n",
    "                                     'hashtag_length_chars', 'hashtag_length_words', 'text_clean_length_chars', 'text_clean_length_words']])\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Convert sentiment to dense format to apply scaler\n",
    "sentiment_dense = sentiment_test.toarray()\n",
    "\n",
    "# Apply Min-Max Scaler\n",
    "scaler = MinMaxScaler()\n",
    "sentiment_normalized = scaler.fit_transform(sentiment_dense)\n",
    "\n",
    "# Convert back to sparse matrix if needed\n",
    "sentiment_test = csr_matrix(sentiment_normalized)\n",
    "\n",
    "text_features_test = vectorizer.transform(test_data['text_clean'])\n",
    "\n",
    "# TF-IDF for 'full_text'\n",
    "full_text_features = full_text_vectorizer.transform(test_data['full_text'])\n",
    "\n",
    "# Replace NaN with an empty string\n",
    "test_data['hashtags'] = test_data['hashtags'].fillna('')\n",
    "\n",
    "hashtag_features_test = hashtag_vectorizer.transform(test_data['hashtags'])\n",
    "\n",
    "topic_probs = lda.transform(text_features_test)\n",
    "# Add topic probabilities to features\n",
    "topic_features = csr_matrix(topic_probs)\n",
    "\n",
    "# Apply One-Hot Encoding (if treating as categorical)\n",
    "test_data['in_reply_to_screen_name'] = test_data['in_reply_to_screen_name'].fillna('')\n",
    "reply_features = reply_vectorizer.transform(test_data['in_reply_to_screen_name'])\n",
    "\n",
    "# Step 2: One-Hot Encode Categorical Features\n",
    "categorical_features_test = encoder.transform(test_data[['country_user', 'gender_user']])\n",
    "test_features = hstack([text_features_test, full_text_features, \n",
    "                        hashtag_features_test, sentiment_test, categorical_features_test, reply_features, topic_features])  # Combine sparse matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2159b793-f24a-416e-8579-e489023a6476",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_predictions = model.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c52a8578-8497-4731-93bd-4c40a3ca1730",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file created: 'submission.csv'\n"
     ]
    }
   ],
   "source": [
    "submission_df = pd.DataFrame({\n",
    "    'ID': test_data['Id'],  # Ensure 'Id' is in the test set\n",
    "    'pol_spec_user': test_predictions  # Predicted political views\n",
    "})\n",
    "\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"Submission file created: 'submission.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0eac104-2605-48ee-9c27-90634d54373a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
